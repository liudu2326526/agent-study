import os
import sys
import asyncio
from contextlib import AsyncExitStack

from dotenv import load_dotenv
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.callbacks import StreamingStdOutCallbackHandler
from langchain_community.chat_message_histories import SQLChatMessageHistory
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, AIMessageChunk

# MCP Imports
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ==========================================
# 1. å®šä¹‰å·¥å…· (Tools)
# ==========================================
@tool
def magic_calculator(a: int, b: int) -> int:
    """
    ä¸€ä¸ªç¥å¥‡çš„è®¡ç®—å™¨ï¼Œå®ƒä¼šå°†ä¸¤ä¸ªæ•°å­—ç›¸åŠ ï¼Œç„¶åä¹˜ä»¥ 2ã€‚
    ç”¨äºæ¼”ç¤ºå·¥å…·è°ƒç”¨ã€‚
    """
    return (a + b) * 2


@tool
def get_weather(city: str) -> str:
    """
    è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ã€‚
    """
    return f"{city} çš„å¤©æ°”æ˜¯æ™´æœ—ï¼Œæ°”æ¸© 25 åº¦ã€‚"


tools = [magic_calculator, get_weather]

# ==========================================
# 2. é…ç½® Memory (SQLite)
# ==========================================
DB_CONNECTION = "sqlite:///memory.db"


def get_chat_history(session_id: str) -> SQLChatMessageHistory:
    """
    è·å–åŸºäº SQLite çš„èŠå¤©è®°å½•ç®¡ç†å™¨ã€‚
    """
    return SQLChatMessageHistory(session_id=session_id, connection=DB_CONNECTION)


ARK_BASE_URL = "https://ark.cn-beijing.volces.com/api/v3"
DEFAULT_MODEL = "deepseek-v3-2-251201"


# ==========================================
# 3. å®šä¹‰æµå¼ç”Ÿæˆå™¨æ–¹æ³•
# ==========================================
async def chat_generator(agent, history: SQLChatMessageHistory, user_input: str):
    """
    æµå¼å¯¹è¯ç”Ÿæˆå™¨ (Async)ã€‚
    
    Args:
        agent: ç¼–è¯‘å¥½çš„ LangGraph Agent
        history: èŠå¤©è®°å½•ç®¡ç†å™¨
        user_input: ç”¨æˆ·è¾“å…¥
        
    Yields:
        str: AI å›å¤çš„æ–‡æœ¬ç‰‡æ®µ
    """
    # 1. å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°å†å²è®°å½•
    history.add_user_message(user_input)
    
    # 2. è·å–å½“å‰ä¸Šä¸‹æ–‡
    current_messages = history.messages
    
    accumulated_content = ""
    
    # 3. ä½¿ç”¨ stream æ¨¡å¼è°ƒç”¨ Agent (Async)
    # stream_mode="messages" ä¼šè¿”å›æ¶ˆæ¯å— (MessageChunk)
    try:
        async for chunk, metadata in agent.astream(
            {"messages": current_messages}, 
            stream_mode="messages"
        ):
            # æˆ‘ä»¬åªå…³å¿ƒ AI çš„å›å¤å†…å®¹ (AIMessageChunk)
            if isinstance(chunk, AIMessageChunk) and chunk.content:
                content = chunk.content
                accumulated_content += content
                yield content
                
    except Exception as e:
        print(f"\nâŒ æµå¼ç”Ÿæˆå‡ºé”™: {e}")
        yield f"[Error: {e}]"
        
    # 4. ä¿å­˜å®Œæ•´å›å¤
    if accumulated_content:
        history.add_ai_message(accumulated_content)

# ==========================================
# 4. ä¸»ç¨‹åº
# ==========================================
async def main():
    # æ£€æŸ¥ API Key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ° OPENAI_API_KEYã€‚")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEY=sk-...")

    # åˆå§‹åŒ– LLM
    try:
        # æ³¨æ„ï¼šä½¿ç”¨ generator yield æ–¹å¼æ—¶ï¼Œé€šå¸¸ä¸éœ€è¦ StreamingStdOutCallbackHandler
        # é™¤éä½ æƒ³åœ¨æ§åˆ¶å°åŒæ—¶ä¹Ÿçœ‹åˆ°è¾“å‡ºã€‚è¿™é‡Œæˆ‘ä»¬å»æ‰å®ƒï¼Œæ¼”ç¤ºçº¯ generator æ§åˆ¶ã€‚
        llm = ChatOpenAI(
            model=DEFAULT_MODEL,
            temperature=0,
            base_url=ARK_BASE_URL,
            api_key=api_key,
            streaming=True, # ä¾ç„¶éœ€è¦å¼€å¯ streaming
            # callbacks=[StreamingStdOutCallbackHandler()] # ç§»é™¤ stdout callback
        )
    except Exception as e:
        print(f"LLM åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # ==========================================
    # MCP å·¥å…·åŠ è½½é€»è¾‘
    # ==========================================
    mcp_tools = []
    
    # ä½¿ç”¨ AsyncExitStack ç®¡ç†å¤šä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ (MCP Sessions)
    async with AsyncExitStack() as stack:
        # ç¤ºä¾‹ï¼šè¿æ¥åˆ°ä¸€ä¸ªæœ¬åœ° MCP æœåŠ¡å™¨
        # server_params = StdioServerParameters(
        #     command="npx",
        #     args=["-y", "@modelcontextprotocol/server-filesystem", "/Users/macbook/Desktop"],
        # )
        
        # å®é™…ä½¿ç”¨æ—¶ï¼Œè¯·å–æ¶ˆæ³¨é‡Šå¹¶é…ç½®æ­£ç¡®çš„ command/args
        mcp_servers = [
            # (StdioServerParameters(command="...", args=[...])) 
        ]
        
        for params in mcp_servers:
            try:
                # è¿æ¥åˆ° MCP æœåŠ¡å™¨
                read, write = await stack.enter_async_context(stdio_client(params))
                session = await stack.enter_async_context(ClientSession(read, write))
                await session.initialize()
                
                # åŠ è½½å·¥å…·
                tools_from_server = await load_mcp_tools(session)
                mcp_tools.extend(tools_from_server)
                print(f"âœ… å·²åŠ è½½ MCP å·¥å…·: {[t.name for t in tools_from_server]}")
                
            except Exception as e:
                print(f"âŒ è¿æ¥ MCP æœåŠ¡å™¨å¤±è´¥ ({params.command}): {e}")

        # åˆå¹¶æ‰€æœ‰å·¥å…·
        all_tools = tools + mcp_tools

        # åˆ›å»º Agent
        print(f"æ­£åœ¨åˆ›å»º Agent (å·¥å…·æ•°: {len(all_tools)})...")
        agent_app = create_agent(llm, all_tools)

        # æ¨¡æ‹Ÿç”¨æˆ·ä¼šè¯
        session_id = "user_session_001"
        history = get_chat_history(session_id)
        
        print(f"\nğŸš€ Agent å·²å¯åŠ¨ (Session ID: {session_id})")
        print(f"ğŸ’¾ è®°å¿†å­˜å‚¨äº: {DB_CONNECTION}")
        print("æ‚¨å¯ä»¥è¾“å…¥é—®é¢˜ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºã€‚\n")

        while True:
            try:
                # ä½¿ç”¨ run_in_executor æ¥é¿å…é˜»å¡ async loop (è™½ç„¶ input æœ¬èº«æ˜¯é˜»å¡çš„)
                # ç®€å•çš„è„šæœ¬ä¸­å¯ä»¥ç›´æ¥ç”¨ inputï¼Œä½†ä¸ºäº†æ›´å¥½çš„ async ä½“éªŒï¼š
                user_input = await asyncio.to_thread(input, "User: ")
                user_input = user_input.strip()
            except EOFError:
                break

            if not user_input:
                continue
                
            if user_input.lower() in ["quit", "exit"]:
                print("å†è§ï¼")
                break

            # 3. è°ƒç”¨ Agent (ä½¿ç”¨ Generator)
            try:
                print("AI: ", end="", flush=True)
                
                async for token in chat_generator(agent_app, history, user_input):
                    print(token, end="", flush=True)
                    
                print("") # æ¢è¡Œ
                    
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    asyncio.run(main())
